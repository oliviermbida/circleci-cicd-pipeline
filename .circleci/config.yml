version: 2.1
orbs: 
  jira: circleci/jira@1.3.1
  slack: circleci/slack@4.10.1

parameters:
#   run_integration_tests:
#     type: boolean
#     default: false
  workflowID:
    type: string
    default: cli803394216630
commands:
# Useful if you don't want to re-run deploy-infrastructure which takes a while.
  check_job:
    description: Stop job if false  
    parameters:
      start_job:
        type: boolean
        default: true        
    steps: 
      - when:
          condition: 
            not: << parameters.start_job >>
          steps:
            - run: circleci-agent step halt      

# destroy-environment:
# Cloudformation is set to rollback to the last successfull update.
# Therefore this is a nuclear option to destroy the entire stack.
# And there will have to be an event loop to check for a signal
# when it is ok to use this option otherwise it will not be possible
# to do so untill a clean up of the failed update is completed.
  destroy-environment:
    description: >
      Destroy back-end and front-end cloudformation stacks given a workflow ID.
      Can only be done by an Admin IAM user.
    parameters:
      workflow_id:
        type: string
        default: <<pipeline.parameters.workflowID>>
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of stack 
        type: string
        default: $CFN_STACKS
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")
            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")
            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

            # delete infrastructure
            if [[ <<parameters.workflow_id>> == $CIRCLE_WORKFLOW_ID ]]; then
            aws cloudformation delete-stack --stack-name <<parameters.stackname>>
            fi
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
#   revert-migrations:
#     description: Revert the last migration if successfully run in the current workflow.
#     parameters:
#       # Add parameter here     
#     steps:
#       - run:
#           name: Revert migrations
#           # Add when this will run
#           command: |
#             # Curl command here to see if there was a successful migration associated with the workflow id, store result in SUCCESS variable
#             SUCCESS = 1
#             if(( $SUCCESS==1 )); 
#             then
#             #  cd ~/project/backend
#             #  npm install
#             #  Add revert code here. You can find this in the Getting Started section.
#               exit 1
#             fi
  check_master:
    description: Stop job if not master branch but continue with other jobs in pipeline          
    steps:        
      - run: 
          name: Check if master 
          command: |
            # If condition is true stop this job but continue with others jobs in the pipeline
            if [ "$CIRCLE_BRANCH" != "master" ]; then
                circleci-agent step halt
            fi
  commit_to_github:
    description: Commit to github
    parameters:
      commit_message:
        type: string
        default: "NO_BUILD Auto commit from CircleCI [skip ci]"
    steps: 
      # - checkout
      # For a self-hosted runner, ensure that you have an ssh-agent on your system 
      # to successfully use the add_ssh_keys step. 
      # The SSH key is written to $HOME/.ssh/id_rsa_<fingerprint>, 
      # where $HOME is the home directory of the user configured to execute jobs, 
      # and <fingerprint> is the fingerprint of the key. 
      # A host entry is also appended to $HOME/.ssh/config, 
      # along with a relevant IdentityFile option to use the key.
      - add_ssh_keys:
          fingerprints:
            - "43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc"
      - run:
          name: Commit to GitHub
          command: |
            if [[ -z "${CIRCLE_PULL_REQUEST}" ]]
            then
              printf "%s" 'github.com ssh-ed25519 43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc' >> "$HOME/.ssh/known_hosts"
              if [ "${HOME}" = "/" ]
              then
                export HOME=$(getent passwd $(id -un) | cut -d: -f6)
              fi
              export GIT_SSH_COMMAND='ssh -i "$HOME/.ssh/id_ed25519" -o UserKnownHostsFile="$HOME/.ssh/known_hosts"'
              echo "Committing to GitHub"

              # use git+ssh instead of https
              git config --global url."ssh://git@github.com".insteadOf "https://github.com" || true
              git config --global gc.auto 0 || true
              git config user.email $USER_EMAIL
              git config user.name $USER_NAME
              git checkout master
              git commit -am "<<parameters.commit_message>>"
              git push origin master
            else
              echo "No commit to GitHub"
            fi
  revert-commit:
    description: >
      To revert a commit on fail , it needs to have a REVERT trigger in commit message.
      Can be useful in testing but not recommend for regular usage because you learn
      from troubleshooting bugs and errors.
    parameters:
      commit_sha:
        type: string
      commit_trigger:
        type: boolean
        default: true
    steps: 
      - when:
          condition: << parameters.commit_trigger >>
          steps:
            - add_ssh_keys:
                fingerprints:
                  - "43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc"
            - run: 
                name: Revert last commit on fail
                command: |
                  commit_message=$(git log -1 HEAD --pretty=format:%s)
                  if [[ $commit_message == *REVERT* ]]; then
                    echo "Revert commit on Fail"

                    printf "%s" 'github.com ssh-ed25519 43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc' >> "$HOME/.ssh/known_hosts"
                    if [ "${HOME}" = "/" ]
                    then
                      export HOME=$(getent passwd $(id -un) | cut -d: -f6)
                    fi
                    export GIT_SSH_COMMAND='ssh -i "$HOME/.ssh/id_ed25519" -o UserKnownHostsFile="$HOME/.ssh/known_hosts"'
                    echo "Committing to GitHub"

                    # use git+ssh instead of https
                    git config --global url."ssh://git@github.com".insteadOf "https://github.com" || true
                    git config --global gc.auto 0 || true
                    git config --global user.email $USER_EMAIL
                    git config --global user.name $USER_NAME

                    # Caution using reset: this may cause push issues as the reverted commit will not exist
                    #git reset --hard <<parameters.commit_sha>> 
                    #git push --force

                    # This reverses last commit but does not delete that commit history
                    git revert --no-commit <<parameters.commit_sha>> 
                    git push --force
                  fi          
# Similar to [skip ci] or [ci skip] in commit message
  cancel-workflow:
    description: Cancel workflow given a commit message to stop it being run automatically
    parameters:
      workflow_id:
        type: string
        default: $CIRCLE_WORKFLOW_ID
      custom-identifier:
        type: string
        default: "NO_BUILD"
    steps: 
      - checkout
      # - run: git submodule sync
      # - run: git submodule update --init
      - run:
          name: Stop automatic builds 
          command: |
            commit_message=$(git log -1 HEAD --pretty=format:%s)
            if [[ $commit_message == *<<parameters.custom-identifier>>* ]]; then
            echo "<<parameters.custom-identifier>> commit, cancelling workflow <<parameters.workflow_id>>"
            curl --request POST \
              --url https://circleci.com/api/v2/workflow/<<parameters.workflow_id>>/cancel \
              --header "Circle-Token: ${CIRCLE_TOKEN}"
            fi
  scan:
    description: >
      Detect bugs and vulnerabilities using sonar scanner
      Requires a sonarcloud account where report is posted
      with the same projectKey
    parameters:
      cache_version:
        default: 1
        description: increment this value if the cache is corrupted and you want to start with a clean cache
        type: integer
      project_root:
        default: .
        description: the root of the project that should be analyzed (relative to the root directory of the repository)
        type: string
      exclusions:
        type: string
        default: "**/*.yaml,**/*.yml"
      host_url:
        type: string
        default: $SONARQUBE_SERVER_URL
      sources:
        type: string
        default: "."
      runner_opts:
        type: string
        default: "-Xms1024m"
      projectKey:
        type: string
        default: "circle-cicd-pipeline"
      organization:
        type: string
        default: $SONARQUBE_SERVER_ORG
    steps:
      - run:
          command: mkdir -p /tmp/cache/scanner
          name: Create cache directory if it doesn't exist
      - restore_cache:
          keys:
            - v<<parameters.cache_version>>-sonarcloud-scanner-4.7.0.2747
      - run:
          name: SonarCloud
          command: |
            set -e
            VERSION=4.7.0.2747
            SCANNER_DIRECTORY=/tmp/cache/scanner
            export SONAR_USER_HOME=$SCANNER_DIRECTORY/.sonar
            OS="linux"
            echo $SONAR_USER_HOME

            export SONAR_RUNNER_OPTS="<< parameters.runner_opts >>"

            if [[ ! -x "$SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner" ]]; then
              curl -Ol https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-$VERSION-$OS.zip
              unzip -qq -o sonar-scanner-cli-$VERSION-$OS.zip -d $SCANNER_DIRECTORY
            fi

            chmod +x $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner
            chmod +x $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/jre/bin/java

            cd <<parameters.project_root>>
            $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner \
            -Dsonar.organization=<< parameters.organization >> \
            -Dsonar.projectKey=<< parameters.projectKey >> \
            -Dsonar.host.url=<< parameters.host_url >> \
            -Dsonar.login=$SONAR_TOKEN \
            -Dsonar.projectBaseDir=<< parameters.project_root >> \
            -Dsonar.sources=<< parameters.sources >> \
            -Dsonar.exclusions="<< parameters.exclusions >>"                  
      - save_cache:
          key: v<<parameters.cache_version>>-sonarcloud-scanner-4.7.0.2747
          paths: /tmp/cache/scanner

# AWS CLI v2           
# Could use the Orb circleci/aws-cli@3.1.1
# Best to know which commands are executed if you are security aware
# You also reduce the overheads of a generic Orb
  install_aws:
    description: Install the AWS CLI via Pip if not already installed.
    parameters:
      binary-dir:
        default: /usr/local/bin
        description: >
          The main aws program in the install directory is symbolically linked to
          the file aws in the specified path. Defaults to /usr/local/bin
        type: string
      install-dir:
        default: /usr/local/aws-cli
        description: >
          Specify the installation directory of AWS CLI. Defaults to
          /usr/local/aws-cli
        type: string
    steps:
      - run:
          command: |
            curl -sSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64$1.zip" -o "awscliv2.zip"
            unzip -q -o awscliv2.zip
            sudo ./aws/install -i "${PARAM_AWS_CLI_INSTALL_DIR}" -b "${PARAM_AWS_CLI_BINARY_DIR}"
            rm -r awscliv2.zip ./aws

            aws --version
          environment:
            PARAM_AWS_CLI_BINARY_DIR: <<parameters.binary-dir>>
            PARAM_AWS_CLI_INSTALL_DIR: <<parameters.install-dir>>
          name: Install AWS CLI v2
# create_changes
# Multi IAM User and roles needed for various steps in creating
# aws infrasturcture.
# Here is where users with different credentials access aws.
# The critical issue is setting up aws cli env variables and config for those iam users when needed.
# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_DEFAULT_REGION could be set in a restricted context or project level
# but it will create an issue of only allowing one user whose credentials are set at that level.
# Another solution will be to create different profiles/credentials in 
# AWS_CONFIG_FILE~/.aws/config and AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials then use them here with a shared workspace. 
  create_changes:
    description: Create stack infrasturcture changes 
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of stack 
        type: string
        default: $CFN_STACKS
      s3bucket:
        description: Name of S3 bucket to store cloudformation artifacts 
        type: string
        default: $CFN_BUCKET
      changename:
        description: Name of stack change set
        type: string 
      version:
        description: Template version
        type: string 
      capabilities:
        description: IAM user 
        type: string
        default: $AWS_IAM   
      type:
        description: Type of change 
        type: string
        default: UPDATE 
    steps:
      - run:
          name: Upload artifacts to S3 
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 

            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY" 

            cd .circleci/files/
            aws cloudformation package --template-file \
              stacks_v<<parameters.version>>.yaml \
              --s3-bucket <<parameters.s3bucket>> \
              --output-template-file stacks_v<<parameters.version>>-packaged.yaml
            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
      - run: 
          name: Wait for local file
          command: |
            # Wait for local file
            while [ ! -f ".circleci/files/stacks_v<<parameters.version>>-packaged.yaml" ] ; do
              echo "..."
            done        
      - run:
          name: Apply <<parameters.changename>> changes
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 

            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY" 
            change_set_ID=$(aws --output text --query "Id" cloudformation create-change-set \
              --stack-name <<parameters.stackname>> \
              --template-body file://.circleci/files/stacks_v<<parameters.version>>-packaged.yaml \
              --capabilities <<parameters.capabilities>> \
              --change-set-name <<parameters.changename>> \
              --change-set-type <<parameters.type>> )

            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
      - run:
          name: <<parameters.changename>> Status 
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 


            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY" 
            while [ 1 ]   # Endless loop.
            do
                change_Status=$(aws --output text --query "Status" cloudformation describe-change-set \
                    --change-set-name <<parameters.changename>> --stack-name <<parameters.stackname>>)
                echo "Change Status: $change_Status"
                if [[ $change_Status == "CREATE_COMPLETE" ]]; then
                    echo "Exiting change-set status: $change_Status"
                    exit 0
                fi
                sleep 5
            done
            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
      - run:
          name: Ensure <<parameters.changename>>  exist
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 

            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"
            aws cloudformation execute-change-set --change-set-name <<parameters.changename>> --stack-name <<parameters.stackname>>
            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
      - run:
          name: Creation Status 
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 

            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"
            while [ 1 ]   # Endless loop.
            do
                stack_Status=$(aws --output text --query "Stacks[0].StackStatus" cloudformation describe-stacks \
                    --stack-name <<parameters.stackname>>)
                echo "Stack Status: $stack_Status"
                if [[ $stack_Status == "CREATE_COMPLETE" || $stack_Status == "UPDATE_COMPLETE"  ]]; then
                    echo "Exiting stack status: $stack_Status"
                    exit 0
                elif [[ $stack_Status == "CREATE_FAILED" || $stack_Status == "ROLLBACK_IN_PROGRESS" || $stack_Status == "ROLLBACK_COMPLETE" || $stack_Status == "UPDATE_ROLLBACK_COMPLETE" || $stack_Status == "DELETE_IN_PROGRESS" ]]; then
                    echo "Exiting stack status: $stack_Status"
                    exit 1
                fi
                sleep 5
            done
            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
  security_changes:
    description: Create IAM roles and Security groups 
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_ADMIN_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_ADMIN_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of stack 
        type: string
        default: $CFN_STACKS
      changename:
        description: Name of stack change set
        type: string 
      type:
        description: Type of change 
        type: string
        default: UPDATE  
      version:
        description: Template version
        type: string 
    steps:     
      - create_changes:
          access_key_id: <<parameters.access_key_id>>
          secret_access_key: <<parameters.secret_access_key>>
          region: <<parameters.region>>
          changename: <<parameters.changename>>
          version: <<parameters.version>>
          type: <<parameters.type>>
# 
defaults: &defaults
  docker:
    - image: circleci/node:13.8.0           
jobs:
  build-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - checkout
      - check_master
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Build front-end
          command: |
            cd ./frontend
            npm i
            npm run build
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify

  build-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - checkout
      - check_master
      - restore_cache:
          keys:
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Back-end build
          command: |
            cd ./backend
            npm i
            npm run build
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify

  test-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Test front-end
          command: |
            cd ./frontend 
            npm i
            npm test 
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}              
      - slack/notify:
          event: pass
          template: basic_success_1
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
      #      - run:
      #         name: Notify failed Tests
      #         command: curl --data fail_tests.log http://example.com/error_logs
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
           
  test-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Test front-end
          command: |
            cd ./backend 
            npm i
            npm test 
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      - slack/notify:
          event: pass
          template: basic_success_1
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
      #      - run:
      #         name: Notify failed Tests
      #         command: curl --data fail_tests.log http://example.com/error_logs
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
      
  scan-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: true
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Scan front-end
          command: |
            cd ./frontend 
            npm install 
            #npm audit --audit-level=critical
            npm audit fix --audit-level=critical --force
    # Will be a good idea to commit the audit fix. This should be run by authorize  $USER_NAME.
      - commit_to_github:
          commit_message: "NO_BUILD frontend npm audit fix"
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }} 
      - slack/notify:
          event: pass
          template: basic_success_1
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #

  scan-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: true
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Scan front-end
          command: |
            cd ./backend 
            npm install
            #npm audit --audit-level=critical 
            npm audit fix --audit-level=critical --force
    # Will be a good idea to commit the audit fix. This should be run by authorize  $USER_NAME.
      - commit_to_github:
          commit_message: "NO_BUILD backend npm audit fix"
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }} 
      - slack/notify:
          event: pass
          template: basic_success_1
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
  scan-sonar:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - checkout
      - check_master
      - scan:
          project_root: .
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
  deploy-infrastructure:
    docker:
      - image: cimg/python:3.9.13-node
    steps:
      - check_job:
          start_job: false
      - install_aws
      - checkout
      - check_master

      # security_changes : 
      # This is run by an IAM user with admin role
      # to setup priviledges required by other steps which are run by other IAM users
      # for simplicity the progres database is also setup by compliance change-set.
      # Ideally there should be another step after compliance change-set for database admin users
      - security_changes:
          # access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          # secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          stackname: $CFN_STACKS
          changename: compliance
          type: CREATE
          version: "1.0"
      # - database_changes:
          # access_key_id: $AWS_DB_ACCESS_KEY_ID
          # secret_access_key: $AWS_DB_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          # stackname: $CFN_STACKS
          # changename: progresdb
          # type: UPDATE
          # version: "1.?"
      # configure aws cli for other users e.g devops team and web developers (use default settings)
      # - configure_aws
      - create_changes:
          # access_key_id: $AWS_USER_ACCESS_KEY_ID
          # secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          stackname: $CFN_STACKS
          changename: deploy-backend
          # type: UPDATE # default setting
          version: "1.1"
      - create_changes:
          # access_key_id: $AWS_USER_ACCESS_KEY_ID
          # secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          stackname: $CFN_STACKS
          changename: deploy-frontend
          version: "1.2"
      - security_changes:
          # access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          # secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          stackname: $CFN_STACKS
          changename: s3policy
          type: UPDATE
          version: "1.3"

# ----------------------------------------------------------
# create_changes command exits on failure notify concern individuals here.
# Cloudformation is set to automatically rollback to last successfull update.
# You can execute a complete rollback of frontend and backend changes here with the commands below but
# it will be better to deal with cloudformation rollbacks at AWS end 
# to avoid an endless loop checking on AWS stack events.

      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
      # rollback changes to security_changes and admin user can delete
      #     - create_changes:
      #         stackname: $CFN_STACKS
      #         changename: rollback-deploy
      #         version: "1.0"  
      # 
  configure-infrastructure:
    docker:
      - image: cimg/python:3.9.13-node
      # build-essential 12.8ubuntu1.1, curl 7.68.0, docker 20.10.14, 
      # docker-compose Docker Compose version v2.4.1 v2.4.1, 
      # dockerize v0.6.1, git 2.36.1, jq 1.6, node 16.15.0, pip 22.0.4, 
      # pipenv 2022.5.2, poetry 1.1.13, pyenv 2.3.0, python2 2.7.18, 
      # python3 3.9.13, ubuntu 20.04.4 LTS, virtualenv 20.14.1, wget 1.20.3, wheel 0.37.1, yarn 1.22.18
    steps:
      - check_job:
          start_job: false
      - install_aws
      - checkout
      - check_master
      # - add_ssh_keys:
      #     fingerprints:
      #       - "<<fingerprint>>"
      # - attach_workspace:
      #     at: ~/
      - run: 
          name: Install ansible
          command: |
            export PIP=$(which pip pip3 | head -1)
            if [[ -n $PIP ]]; then
              if which sudo > /dev/null; then
                sudo $PIP install ansible --upgrade
              else
                $PIP install ansible --upgrade --user
              fi 
            else
              echo "Unable to install Ansible. Please install pip."
              exit 1
            fi
      - run: 
          name: Configure ansible
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")
            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")
            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

            # Access secrets which can be rotated by the authorized IAM user
            # without the need of manually deleting and adding new env variables in Circleci
            
            EC2_USER=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleSSH \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."user")

            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.workflowID>>') 

            # Add back-end connection details to ansible inventory
            # default host inventory /etc/ansible/hosts
            # ansible_ssh_private_key_file=./web-key.pem

            if [[ $EC2_HOST != "" && -f ".circleci/ansible/inventory.ini" ]]; then
              echo "web ansible_host=$EC2_HOST ansible_connection=ssh  ansible_user=$EC2_USER" | tee -a  .circleci/ansible/inventory.ini >> /dev/null
            fi          
            chmod 0600 ".circleci/ansible/inventory.ini"

            # SSH connection configuration
            if [ "${HOME}" = "/" ]
            then
              export HOME=$(getent passwd $(id -un) | cut -d: -f6)
            fi
            printf "%s" '$EC2_HOST ssh-ed25519 QbM8rDBaZ9yajNovvO09gv+ks71u1c1y0C4S6Bt39CE' >> "$HOME/.ssh/known_hosts"
            chmod 0600 "$HOME/.ssh/known_hosts"
            if [ -f "$HOME/.ssh/id_ed25519" ]; then
              rm -f "$HOME/.ssh/id_ed25519"
            fi

            aws ssm get-parameter \
                --name /aws/reference/secretsmanager/udapeople_ssh_key \
                --with-decryption --output text --query "Parameter.Value" > "$HOME/.ssh/id_ed25519"
            chmod 0700 "$HOME/.ssh/id_ed25519" 

            # cleanup
            unset EC2_USER
            unset EC2_HOST
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: $AWS_USER_ACCESS_KEY_ID
            PARAM_AWS_CLI_REGION: $AWS_DEFAULT_REGION
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: $AWS_USER_SECRET_ACCESS_KEY           
      - run:
          name: Configure server
          command: |
            #        
            ansible-playbook  -i .circleci/ansible/inventory.ini .circleci/ansible/configure-server.yml
      # - persist_to_workspace:
      #     root: ~/
      #     paths:
      #       - project/.circleci/ansible/inventory.ini
      #       - .ssh/id_ed25519
      #       - .ssh/known_hosts

      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
  run-migrations:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - install_aws
      - checkout
      - check_master   
      # - restore_cache:
      #     keys: 
      #       - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      #       # fallback to using the latest cache 
      #       - v1-backend-deps-
      - run:
          name: Run migrations
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")
            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")
            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

            TYPEORM_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")

            TYPEORM_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password")

            TYPEORM_DATABASE=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."dbname")

            TYPEORM_PORT=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.workflowID>> \
                --output text --query "DBInstances[*].Endpoint.Port")

            TYPEORM_HOST=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.workflowID>> \
                --output text --query "DBInstances[*].Endpoint.Address")
            cd ./backend 
            npm cache clean --force
            rm -rf node_modules
            rm -f package-lock.json
            npm i
            npm run migrations

            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
            unset TYPEORM_USERNAME
            unset TYPEORM_PASSWORD
            unset TYPEORM_DATABASE
            unset TYPEORM_HOST
            unset TYPEORM_PORT
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: $AWS_ADMIN_ACCESS_KEY_ID
            PARAM_AWS_CLI_REGION: $AWS_DEFAULT_REGION
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: $AWS_ADMIN_SECRET_ACCESS_KEY 
#       - run:
#           name: Send migration results to memstash
#           command: |
#             # Your code here
#             exit 1
      # - save_cache:
      #     paths: 
      #       - backend/node_modules
      #     key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}

      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
  deploy-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: false
      - install_aws
      - checkout
      - check_master
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Get backend url and build
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")
            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")
            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.workflowID>>') 
            #
            export API_URL="http://${EC2_HOST}:3030"
            #echo "${API_URL}" 
            cd ./frontend
            npm i
            npm run build
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: $AWS_USER_ACCESS_KEY_ID
            PARAM_AWS_CLI_REGION: $AWS_DEFAULT_REGION
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: $AWS_USER_SECRET_ACCESS_KEY 
      - run: 
          name: Deploy frontend objects
          command: |+
            aws s3 sync \
              ./frontend/dist s3://udapeoples3-<<pipeline.parameters.workflowID>>/ --delete \
              --acl public-read \
              --cache-control "max-age=86400"   
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}   
      ##Below are options to deal with a failed job 
      # 
      - when:
          condition: on_fail
          steps:
            - slack/notify:
                event: fail
                template: basic_fail_1
            # - jira/notify
           
#   deploy-backend:
#     docker:
#       # Docker image here that supports Ansible
#     steps:
#       # Checkout code from git
#       # Add ssh keys with fingerprint
#       # attach workspace
#       - run:
#           name: Install dependencies
#           command: |
#             # your code here
#       - run:
#           name: Deploy backend
#           command: |
#            ansible-playbook  -i .circleci/ansible/inventory.ini .circleci/ansible/deploy-backend.yml
#       # Here's where you will add some code to rollback on failure  

#   smoke-test:
#     docker:
#       # Lightweight Docker image 
#     steps:
#       # Checkout code from git
#       - run:
#           name: Install dependencies
#           command: |
#             # your code here
#       - run:
#           name: Get backend url
#           command: |
#             # your code here
#       - run:
#           name: Backend smoke test.
#           command: |
#             # your code here
#       - run:
#           name: Frontend smoke test.
#           command: |
#             # your code here
#       # Here's where you will add some code to rollback on failure  

#   cloudfront-update:
#     docker:
#       # Docker image here that supports AWS CLI
#     steps:
#       # Checkout code from git
#       - run:
#           name: Install dependencies
#           command: |
#             # your code here
#       - run:
#           name: Update cloudfront distribution
#           command: |
#             # your code here
#       # Here's where you will add some code to rollback on failure  

# cleanup:
#     docker:
#       # Docker image here
#     steps:
#       # Checkout code from git
#       - run:
#           name: Get old stack workflow id
#           command: |
#             # your code here
#             export OldWorkflowID="the id here"
#             export STACKS=[] #put the list of stacks here
#       - run:
#           name: Remove old stacks and files
#           command: |
#             if [[ "${STACKS[@]}" =~ "${OldWorkflowID}" ]]
#             then
#               # your code here
#             fi
workflows:
  default:
    jobs:
      - build-frontend:
          pre-steps: # Check commit message if NO_BUILD
            - cancel-workflow      
      - build-backend:
          pre-steps: # Check commit message if NO_BUILD
            - cancel-workflow
      - test-frontend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-frontend
          # filters:
          #   branches:
          #     only: master
      - test-backend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-backend
          # filters:
          #   branches:
          #     only: master
      - scan-backend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-backend
      - scan-frontend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-frontend
      - scan-sonar:
          post-steps:
            - when:
                condition: on_fail
                steps:
                - jira/notify:
                    environment_type: testing
                    job_type: build
          requires: 
            - build-frontend
            - build-backend
      - deploy-infrastructure:
          requires: 
            - test-frontend
            - test-backend
            - scan-frontend
            - scan-backend
          context:
            - org-global
            - aws-context
      - configure-infrastructure:
          requires: 
            - deploy-infrastructure
          context:
            - org-global
            - aws-context
      - run-migrations:
          requires: 
            - configure-infrastructure
          context:
            - org-global
            - aws-context
      - deploy-frontend:
          requires: 
            - run-migrations
          context:
            - org-global
            - aws-context
      # - deploy-backend:
      #     requires: [run-migrations]
      # - smoke-test:
      #     requires: [deploy-backend, deploy-frontend]
      # - cloudfront-update:
      #     requires: [smoke-test]
      # - cleanup:
      #     requires: [cloudfront-update]
  nightly:
    jobs:
      - scan-frontend
      - scan-backend
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
  # integration_tests:
  #   when: << pipeline.parameters.run_integration_tests >>
  #   jobs:
  #     - build-frontend
  #     - build-backend
  #     - deploy-infrastructure:
  #         requires: 
  #           - build-frontend
  #           - build-backend
  #         context:
  #           - org-global
  #           - aws-context
  #     - configure-infrastructure:
  #         requires: 
  #           - deploy-infrastructure
  #     - run-migrations:
  #         requires: 
  #           - configure-infrastructure
  #     - deploy-frontend:
  #         requires: 
  #           - run-migrations
  #     - deploy-backend:
  #         requires: 
  #           - run-migrations
  #     - cloudfront-update:
  #         requires: 
  #         - run-migrations
  #     - cleanup:
  #         requires: 
  #           - cloudfront-update