version: 2.1
orbs: 
  jira: circleci/jira@1.3.1
  slack: circleci/slack@4.10.1

parameters:
  resourceID:
    description: Unique resource identifier
    type: string
    default: cli803394216630
  environment:
    type: string
    default: cfn-Blue
#   run_integration_tests:
#     type: boolean
#     default: false
  run_smoke_test:
    type: boolean
    default: true
  commit:
    type: boolean
    default: false
  run_build_frontend:
    type: boolean
    default: false
  run_build_backend:
    type: boolean
    default: false
  run_test_frontend:
    type: boolean
    default: false
  run_test_backend:
    type: boolean
    default: false
  run_scan_frontend:
    type: boolean
    default: false
  run_scan_backend:
    type: boolean
    default: false
  run_scan_sonar:
    type: boolean
    default: false
  run_deploy_infrastructure:
    type: boolean
    default: true
  run_configure_infrastructure:
    type: boolean
    default: true
  run_migrations:
    type: boolean
    default: true
  run_deploy_frontend:
    type: boolean
    default: true
  run_deploy_backend:
    type: boolean
    default: true
  run_cloudfront_update:
    type: boolean
    default: false
# Cloudfront
  use_cloudfront: 
    type: boolean
    default: true
  cloudfront_distribution_id:
    type: string
    default: ""
  cloudfront_origin_id:
    type: string
    default: udapeoples3-cli803394216630
# ansible config
  ans_inventory_file:
    type: string
    default: .circleci/ansible/inventory.ini
commands:
# Useful if you don't want to re-run deploy-infrastructure which takes a while.
  check_job:
    description: Stop job if false  
    parameters:
      start_job:
        type: boolean
        default: true        
    steps: 
      - when:
          condition: 
            not: << parameters.start_job >>
          steps:
            - run: circleci-agent step halt      

# destroy-environment:
# Cloudformation is set to rollback to the last successfull update.
# Therefore this is a nuclear option to destroy the entire stack.
# And there will have to be an event loop to check for a signal
# when it is ok to use this option otherwise it will not be possible
# to do so untill a clean up of the failed update is completed.
  destroy-environment:
    description: >
      Destroy back-end and front-end cloudformation stacks given a workflow ID.
      Can only be done by an Admin IAM user.
    parameters:
      workflow_id:
        type: string
        default: <<pipeline.parameters.resourceID>>
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of stack 
        type: string
        default: <<pipeline.parameters.environment>>
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |

            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"

            # delete infrastructure
            if [[ <<parameters.workflow_id>> == $CIRCLE_WORKFLOW_ID ]]; then
            aws cloudformation delete-stack --stack-name <<parameters.stackname>>
            fi

#   revert-migrations:
#     description: Revert the last migration if successfully run in the current workflow.
#     parameters:
#       # Add parameter here     
#     steps:
#       - run:
#           name: Revert migrations
#           # Add when this will run
#           command: |
#             # Curl command here to see if there was a successful migration associated with the workflow id, store result in SUCCESS variable
#             SUCCESS = 1
#             if(( $SUCCESS==1 )); 
#             then
#             #  cd ~/project/backend
#             #  npm install
#             #  Add revert code here. You can find this in the Getting Started section.
#               exit 1
#             fi
  check_master:
    description: Stop job if not master branch but continue with other jobs in pipeline          
    steps:        
      - run: 
          name: Check if master 
          command: |
            # If condition is true stop this job but continue with others jobs in the pipeline
            if [ "$CIRCLE_BRANCH" != "master" ]; then
                circleci-agent step halt
            fi
  commit_to_github:
    description: Commit to github
    parameters:
      commit_message:
        type: string
        default: "NO_BUILD Auto commit from CircleCI [skip ci]"
      commit:
          type: boolean
          default: false 
    steps: 
      # - checkout
      # For a self-hosted runner, ensure that you have an ssh-agent on your system 
      # to successfully use the add_ssh_keys step. 
      # The SSH key is written to $HOME/.ssh/id_rsa_<fingerprint>, 
      # where $HOME is the home directory of the user configured to execute jobs, 
      # and <fingerprint> is the fingerprint of the key. 
      # A host entry is also appended to $HOME/.ssh/config, 
      # along with a relevant IdentityFile option to use the key.
      - when:
          condition: 
              not: <<parameters.commit>>
          steps:
            - add_ssh_keys:
                fingerprints:
                  - "43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc"
            - run:
                name: Commit to GitHub
                command: |
                  if [[ -z "${CIRCLE_PULL_REQUEST}" ]]
                  then
                    printf "%s" 'github.com ssh-ed25519 43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc' >> "$HOME/.ssh/known_hosts"
                    if [ "${HOME}" = "/" ]
                    then
                      export HOME=$(getent passwd $(id -un) | cut -d: -f6)
                    fi
                    export GIT_SSH_COMMAND='ssh -i "$HOME/.ssh/id_ed25519" -o UserKnownHostsFile="$HOME/.ssh/known_hosts"'
                    echo "Committing to GitHub"

                    # use git+ssh instead of https
                    git config --global url."ssh://git@github.com".insteadOf "https://github.com" || true
                    git config --global gc.auto 0 || true
                    git config user.email $USER_EMAIL
                    git config user.name $USER_NAME
                    git checkout master
                    git pull origin master
                    git commit --allow-empty -am "<<parameters.commit_message>>"
                    git push origin master
                  else
                    echo "No commit to GitHub"
                  fi
  revert-commit:
    description: >
      To revert a commit on fail , it needs to have a REVERT trigger in commit message.
      Can be useful in testing but not recommend for regular usage because you learn
      from troubleshooting bugs and errors.
    parameters:
      commit_sha:
        type: string
      commit_trigger:
        type: boolean
        default: true
    steps: 
      - when:
          condition: << parameters.commit_trigger >>
          steps:
            - add_ssh_keys:
                fingerprints:
                  - "43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc"
            - run: 
                name: Revert last commit on fail
                command: |
                  commit_message=$(git log -1 HEAD --pretty=format:%s)
                  if [[ $commit_message == *REVERT* ]]; then
                    echo "Revert commit on Fail"

                    printf "%s" 'github.com ssh-ed25519 43:08:ce:f6:6f:04:b8:d2:53:1b:0c:fd:19:8f:2f:cc' >> "$HOME/.ssh/known_hosts"
                    if [ "${HOME}" = "/" ]
                    then
                      export HOME=$(getent passwd $(id -un) | cut -d: -f6)
                    fi
                    export GIT_SSH_COMMAND='ssh -i "$HOME/.ssh/id_ed25519" -o UserKnownHostsFile="$HOME/.ssh/known_hosts"'
                    echo "Committing to GitHub"

                    # use git+ssh instead of https
                    git config --global url."ssh://git@github.com".insteadOf "https://github.com" || true
                    git config --global gc.auto 0 || true
                    git config --global user.email $USER_EMAIL
                    git config --global user.name $USER_NAME

                    # Caution using reset: this may cause push issues as the reverted commit will not exist
                    #git reset --hard <<parameters.commit_sha>> 
                    #git push --force

                    # This reverses last commit but does not delete that commit history
                    git revert --no-commit <<parameters.commit_sha>> 
                    git push --force
                  fi          
# Similar to [skip ci] or [ci skip] in commit message
  cancel-workflow:
    description: Cancel workflow given a commit message to stop it being run automatically
    parameters:
      workflow_id:
        type: string
        default: $CIRCLE_WORKFLOW_ID
      custom-identifier:
        type: string
        default: "NO_BUILD"
    steps: 
      - checkout
      # - run: git submodule sync
      # - run: git submodule update --init
      - run:
          name: Stop automatic builds 
          command: |
            commit_message=$(git log -1 HEAD --pretty=format:%s)
            if [[ $commit_message == *<<parameters.custom-identifier>>* ]]; then
            echo "<<parameters.custom-identifier>> commit, cancelling workflow <<parameters.workflow_id>>"
            curl --request POST \
              --url https://circleci.com/api/v2/workflow/<<parameters.workflow_id>>/cancel \
              --header "Circle-Token: ${CIRCLE_TOKEN}"
            fi
  scan:
    description: >
      Detect bugs and vulnerabilities using sonar scanner
      Requires a sonarcloud account where report is posted
      with the same projectKey
    parameters:
      cache_version:
        default: 1
        description: increment this value if the cache is corrupted and you want to start with a clean cache
        type: integer
      project_root:
        default: .
        description: the root of the project that should be analyzed (relative to the root directory of the repository)
        type: string
      exclusions:
        type: string
        default: "**/*.yaml,**/*.yml"
      host_url:
        type: string
        default: $SONARQUBE_SERVER_URL
      sources:
        type: string
        default: "."
      runner_opts:
        type: string
        default: "-Xms1024m"
      projectKey:
        type: string
        default: "circle-cicd-pipeline"
      organization:
        type: string
        default: $SONARQUBE_SERVER_ORG
    steps:
      - run:
          command: mkdir -p /tmp/cache/scanner
          name: Create cache directory if it doesn't exist
      - restore_cache:
          keys:
            - v<<parameters.cache_version>>-sonarcloud-scanner-4.7.0.2747
      - run:
          name: SonarCloud
          command: |
            set -e
            VERSION=4.7.0.2747
            SCANNER_DIRECTORY=/tmp/cache/scanner
            export SONAR_USER_HOME=$SCANNER_DIRECTORY/.sonar
            OS="linux"
            echo $SONAR_USER_HOME

            export SONAR_RUNNER_OPTS="<< parameters.runner_opts >>"

            if [[ ! -x "$SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner" ]]; then
              curl -Ol https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-$VERSION-$OS.zip
              unzip -qq -o sonar-scanner-cli-$VERSION-$OS.zip -d $SCANNER_DIRECTORY
            fi

            chmod +x $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner
            chmod +x $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/jre/bin/java

            cd <<parameters.project_root>>
            $SCANNER_DIRECTORY/sonar-scanner-$VERSION-$OS/bin/sonar-scanner \
            -Dsonar.organization=<< parameters.organization >> \
            -Dsonar.projectKey=<< parameters.projectKey >> \
            -Dsonar.host.url=<< parameters.host_url >> \
            -Dsonar.login=$SONAR_TOKEN \
            -Dsonar.projectBaseDir=<< parameters.project_root >> \
            -Dsonar.sources=<< parameters.sources >> \
            -Dsonar.exclusions="<< parameters.exclusions >>"                  
      - save_cache:
          key: v<<parameters.cache_version>>-sonarcloud-scanner-4.7.0.2747
          paths: /tmp/cache/scanner

# AWS CLI v2           
# Could use the Orb circleci/aws-cli@3.1.1
# Best to know which commands are executed if you are security aware
# You also reduce the overheads of a generic Orb
  install_aws:
    description: Install the AWS CLI via Pip if not already installed.
    parameters:
      binary-dir:
        default: /usr/local/bin
        description: >
          The main aws program in the install directory is symbolically linked to
          the file aws in the specified path. Defaults to /usr/local/bin
        type: string
      install-dir:
        default: /usr/local/aws-cli
        description: >
          Specify the installation directory of AWS CLI. Defaults to
          /usr/local/aws-cli
        type: string
    steps:
      - run:
          command: |
            curl -sSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64$1.zip" -o "awscliv2.zip"
            unzip -q -o awscliv2.zip
            sudo ./aws/install -i "${PARAM_AWS_CLI_INSTALL_DIR}" -b "${PARAM_AWS_CLI_BINARY_DIR}"
            rm -r awscliv2.zip ./aws

            aws --version
          environment:
            PARAM_AWS_CLI_BINARY_DIR: <<parameters.binary-dir>>
            PARAM_AWS_CLI_INSTALL_DIR: <<parameters.install-dir>>
          name: Install AWS CLI v2
  configure_aws:
    description: >
      configure aws credentials
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
    steps: 
      - run:
          name: Configure aws 
          command: |
            # AWS CLI supported environment variables
            AWS_ACCESS_KEY_ID=$(eval echo "$PARAM_AWS_CLI_ACCESS_KEY_ID")

            AWS_SECRET_ACCESS_KEY=$(eval echo "$PARAM_AWS_CLI_SECRET_ACCESS_KEY")

            AWS_DEFAULT_REGION=$(eval echo "$PARAM_AWS_CLI_REGION")

            # configure aws for this job
            aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" 
            aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY" 

            # cleanup
            unset AWS_ACCESS_KEY_ID
            unset AWS_USER_SECRET_ACCESS_KEY
          environment:
            PARAM_AWS_CLI_ACCESS_KEY_ID: <<parameters.access_key_id>>
            PARAM_AWS_CLI_REGION: <<parameters.region>>
            PARAM_AWS_CLI_SECRET_ACCESS_KEY: <<parameters.secret_access_key>>
# Ansible
  install_ansible:
    description: >
      Check if ansible exist
    steps: 
      - run: 
          name: Install ansible
          command: |
            export PIP=$(which pip pip3 | head -1)
            if [[ -n $PIP ]]; then
              if which sudo > /dev/null; then
                sudo $PIP install ansible --upgrade
              else
                $PIP install ansible --upgrade --user
              fi 
            else
              if which sudo > /dev/null; then
                sudo apt update
                sudo apt install python3-pip
                sudo pip3 install ansible --upgrade
              else 
                apt update
                apt install python3-pip
                pip3 install ansible --upgrade
              fi
            fi 
  configure_ansible:
    description: >
      Configure ansible ssh with aws secrets.
      IAM Admin user credentials required to share key.
      Requires aws cli
    steps: 
      - run: 
          name: Configure ansible
          command: |
            # SSH connection configuration
            if [ "${HOME}" = "/" ]
            then
              export HOME=$(getent passwd $(id -un) | cut -d: -f6)
            fi
            printf "%s" '$EC2_HOST ssh-ed25519 QbM8rDBaZ9yajNovvO09gv+ks71u1c1y0C4S6Bt39CE' >> "$HOME/.ssh/known_hosts"
            chmod 0600 "$HOME/.ssh/known_hosts"
            if [ -f "$HOME/.ssh/id_ed25519" ]; then
              rm -f "$HOME/.ssh/id_ed25519"
            fi

            #access ssh key from aws secrets
            aws ssm get-parameter \
                --name /aws/reference/secretsmanager/udapeople_ssh_key \
                --with-decryption --output text --query "Parameter.Value" > "$HOME/.ssh/id_ed25519"
            chmod 0700 "$HOME/.ssh/id_ed25519" 

# create_changes
# Multi IAM User and roles needed for various steps in creating
# aws infrasturcture.
# Here is where users with different credentials access aws.
# The critical issue is setting up aws cli env variables and config for those iam users when needed.
# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_DEFAULT_REGION could be set in a restricted context or project level
# but it will create an issue of only allowing one user whose credentials are set at that level.
# Another solution will be to create different profiles/credentials in 
# AWS_CONFIG_FILE~/.aws/config and AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials then use them here with a shared workspace. 
  create_changes:
    description: Create stack infrasturcture changes 
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_USER_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_USER_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of top level stack 
        type: string
      use_cloudfront: 
        description: Create cloudfront
      resourceID: 
        description: Unique resource identifier
      s3bucket:
        description: Name of S3 bucket to store cloudformation artifacts 
        type: string
        default: $CFN_BUCKET
      changename:
        description: Name of stack change set
        type: string 
      version:
        description: Template version
        type: string 
      capabilities:
        description: IAM user 
        type: string
        default: $AWS_IAM   
      type:
        description: Type of change 
        type: string
        default: UPDATE 
    steps:
      - configure_aws:
          access_key_id: <<parameters.access_key_id>>
          secret_access_key: <<parameters.secret_access_key>>
          region: <<parameters.region>>
      - run:
          name: Upload artifacts to S3 
          command: |
            cd .circleci/files/
            aws cloudformation package --template-file \
              stacks_v<<parameters.version>>.yaml \
              --s3-bucket <<parameters.s3bucket>> \
              --output-template-file stacks_v<<parameters.version>>-packaged.yaml
      - run: 
          name: Wait for local file
          command: |
            # Wait for local file
            while [ ! -f ".circleci/files/stacks_v<<parameters.version>>-packaged.yaml" ] ; do
              echo "..."
            done        
      - run:
          name: Apply <<parameters.changename>> changes
          command: |
            change_set_ID=$(aws --output text --query "Id" cloudformation create-change-set \
              --stack-name <<parameters.stackname>> \
              --template-body file://.circleci/files/stacks_v<<parameters.version>>-packaged.yaml \
              --parameters ParameterKey=Use_Cloudfront, ParameterValue=$PARAM_USE_CLOUDFRONT, ParameterKey=resourceID, ParameterValue=$PARAM_UNIQUE_ID\
              --capabilities <<parameters.capabilities>> \
              --change-set-name <<parameters.changename>> \
              --change-set-type <<parameters.type>> )
          environment: 
            PARAM_USE_CLOUDFRONT: <<parameters.use_cloudfront>>
            PARAM_UNIQUE_ID: <<parameters.resourceID>>
      - run:
          name: <<parameters.changename>> Status 
          command: |
            while [ 1 ]   # Endless loop.
            do
                change_Status=$(aws --output text --query "Status" cloudformation describe-change-set \
                    --change-set-name <<parameters.changename>> --stack-name <<parameters.stackname>>)
                echo "Change Status: $change_Status"
                if [[ $change_Status == "CREATE_COMPLETE" ]]; then
                    echo "Exiting change-set status: $change_Status"
                    exit 0
                fi
                sleep 5
            done
      - run:
          name: Ensure <<parameters.changename>>  exist
          command: |
            aws cloudformation execute-change-set --change-set-name <<parameters.changename>> --stack-name <<parameters.stackname>>
      - run:
          name: Creation Status 
          command: |
            while [ 1 ]   # Endless loop.
            do
                stack_Status=$(aws --output text --query "Stacks[0].StackStatus" cloudformation describe-stacks \
                    --stack-name <<parameters.stackname>>)
                echo "Stack Status: $stack_Status"
                if [[ $stack_Status == "CREATE_COMPLETE" || $stack_Status == "UPDATE_COMPLETE"  ]]; then
                    echo "Exiting stack status: $stack_Status"
                    exit 0
                elif [[ $stack_Status == "CREATE_FAILED" || $stack_Status == "ROLLBACK_IN_PROGRESS" || $stack_Status == "ROLLBACK_COMPLETE" || $stack_Status == "UPDATE_ROLLBACK_COMPLETE" || $stack_Status == "DELETE_IN_PROGRESS" ]]; then
                    echo "Exiting stack status: $stack_Status"
                    exit 1
                fi
                sleep 5
            done
      - run:
          name: Cleanup aws config
          command: |
            # cleanup
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
  security_changes:
    description: Create IAM roles and Security groups 
    parameters:
      access_key_id: 
        type: string
        description: AWS access key Id
        default: $AWS_ADMIN_ACCESS_KEY_ID
      secret_access_key: 
        type: string
        description: AWS secret access key
        default: $AWS_ADMIN_SECRET_ACCESS_KEY
      region: 
        type: string
        description: AWS default region
        default: $AWS_DEFAULT_REGION
      stackname:
        description: Name of top level stack 
        type: string
      use_cloudfront: 
        description: Create cloudfront
      resourceID: 
        description: Unique resource identifier
      changename:
        description: Name of stack change set
        type: string 
      type:
        description: Type of change 
        type: string
        default: UPDATE  
      version:
        description: Template version
        type: string 
    steps:     
      - create_changes:
          access_key_id: <<parameters.access_key_id>>
          secret_access_key: <<parameters.secret_access_key>>
          region: <<parameters.region>>
          resourceID: <<parameters.resourceID>>
          stackname: <<parameters.stackname>>
          use_cloudfront: <<parameters.use_cloudfront>>
          changename: <<parameters.changename>>
          version: <<parameters.version>>
          type: <<parameters.type>>
# 
defaults: &defaults
  docker:
    - image: circleci/node:13.8.0          
jobs:
  build-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_build_frontend>>
      - checkout
      - check_master
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Build front-end
          working_directory: ./frontend
          command: |
            npm i
            npm run build
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify

  build-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_build_backend>>
      - checkout
      - check_master
      - restore_cache:
          keys:
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Back-end build
          working_directory: ./backend
          command: |
            npm i
            npm run build
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      ##You can revert/reset to the previous commit using its commit_sha
      #       - revert-commit:
      #           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify

  test-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_test_frontend>>
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Test front-end
          working_directory: ./frontend
          command: |
            npm i
            npm test 
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}              
      - slack/notify:
          event: pass
          template: basic_success_1

#      - run:
#         name: Notify failed Tests
#         command: curl --data fail_tests.log http://example.com/error_logs
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
##You can revert/reset to the previous commit using its commit_sha
#       - revert-commit:
#           commit_sha: <<pipeline.parameters.commit_sha>>  
      #
           
  test-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_test_backend>>
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Test front-end
          working_directory: ./backend
          command: |
            npm i
            npm test 
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      # - slack/notify:
      #     event: pass
      #     template: basic_success_1
#      - run:
#         name: Notify failed Tests
#         command: curl --data fail_tests.log http://example.com/error_logs
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
##You can revert/reset to the previous commit using its commit_sha
#       - revert-commit:
#           commit_sha: <<pipeline.parameters.commit_sha>>  
#
      
  scan-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_scan_frontend>>
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Scan front-end
          working_directory: ./frontend
          command: |
            npm install 
            #npm audit --audit-level=critical
            npm audit fix --audit-level=critical --force
    # Will be a good idea to commit the audit fix. This should be run by authorize  $USER_NAME.
      - commit_to_github:
          commit_message: "NO_BUILD frontend npm audit fix"
          commit: <<pipeline.parameters.commit>>
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }} 
      # - slack/notify:
      #     event: pass
      #     template: basic_success_1

      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
##You can revert/reset to the previous commit using its commit_sha
#       - revert-commit:
#           commit_sha: <<pipeline.parameters.commit_sha>>  
#

  scan-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_scan_backend>>
      - checkout
      - check_master
      # Restore from cache
      - restore_cache:
          keys: 
            - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-backend-deps-
      - run:
          name: Scan front-end
          working_directory: ./backend
          command: |
            npm install
            #npm audit --audit-level=critical 
            npm audit fix --audit-level=critical --force
    # Will be a good idea to commit the audit fix. This should be run by authorize  $USER_NAME.
      - commit_to_github:
          commit_message: "NO_BUILD backend npm audit fix"
          commit: <<pipeline.parameters.commit>>
      - save_cache:
          paths: 
            - backend/node_modules
          key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }} 
      # - slack/notify:
      #     event: pass
      #     template: basic_success_1

      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
##You can revert/reset to the previous commit using its commit_sha
#       - revert-commit:
#           commit_sha: <<pipeline.parameters.commit_sha>>  
#
  scan-sonar:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_scan_sonar>>
      - checkout
      - check_master
      - scan:
          project_root: .

      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
##You can revert/reset to the previous commit using its commit_sha
#       - revert-commit:
#           commit_sha: <<pipeline.parameters.commit_sha>>  
#
  deploy-infrastructure:
    docker:
      - image: cimg/python:3.9.13-node
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_deploy_infrastructure>>
      - install_aws
      - checkout
      - check_master

      # security_changes : 
      # This is run by an IAM user with admin role
      # to setup priviledges required by other steps which are run by other IAM users
      # for simplicity the progres database is also setup by compliance change-set.
      # Ideally there should be another step after compliance change-set for database admin users
      - security_changes:
          # access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          # secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          resourceID: <<pipeline.parameters.resourceID>>
          use_cloudfront: <<pipeline.parameters.use_cloudfront>>
          stackname: <<pipeline.parameters.environment>>
          changename: compliance
          type: CREATE
          version: "1.0"
      # - database_changes:
          # access_key_id: $AWS_DB_ACCESS_KEY_ID
          # secret_access_key: $AWS_DB_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          # stackname: <<pipeline.parameters.environment>>
          # changename: progresdb
          # type: UPDATE
          # version: "1.?"
      # configure aws cli for other users e.g devops team and web developers (use default settings)
      # - configure_aws
      - create_changes:
          # access_key_id: $AWS_USER_ACCESS_KEY_ID
          # secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          resourceID: <<pipeline.parameters.resourceID>>
          use_cloudfront: <<pipeline.parameters.use_cloudfront>>
          stackname: <<pipeline.parameters.environment>>
          changename: deploy-backend
          # type: UPDATE # default setting
          version: "1.1"
      - create_changes:
          # access_key_id: $AWS_USER_ACCESS_KEY_ID
          # secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          resourceID: <<pipeline.parameters.resourceID>>
          use_cloudfront: <<pipeline.parameters.use_cloudfront>>
          stackname: <<pipeline.parameters.environment>>
          changename: deploy-frontend
          version: "1.2"
      - security_changes:
          # access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          # secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
          # region: $AWS_DEFAULT_REGION
          resourceID: <<pipeline.parameters.resourceID>>
          use_cloudfront: <<pipeline.parameters.use_cloudfront>>
          stackname: <<pipeline.parameters.environment>>
          changename: s3policy
          type: UPDATE
          version: "1.3"

# ----------------------------------------------------------
# create_changes command exits on failure notify concern individuals here.
# Cloudformation is set to automatically rollback to last successfull update.
# You can also rerun this job in isolation by setting check_job parameter start_job to true
# and all other jobs to false. This means you can setup the infrastructure and
# disable this job in the next pipeline

        - slack/notify:
            event: fail
            template: basic_fail_1
        # - jira/notify
  # rollback changes to security_changes and admin user can delete
  #     - create_changes:
  #         stackname: <<pipeline.parameters.environment>>
  #         changename: rollback-deploy
  #         version: "1.0"  
  # 
  configure-infrastructure:
    docker:
      - image: cimg/python:3.9.13-node
      # build-essential 12.8ubuntu1.1, curl 7.68.0, docker 20.10.14, 
      # docker-compose Docker Compose version v2.4.1 v2.4.1, 
      # dockerize v0.6.1, git 2.36.1, jq 1.6, node 16.15.0, pip 22.0.4, 
      # pipenv 2022.5.2, poetry 1.1.13, pyenv 2.3.0, python2 2.7.18, 
      # python3 3.9.13, ubuntu 20.04.4 LTS, virtualenv 20.14.1, wget 1.20.3, wheel 0.37.1, yarn 1.22.18
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_configure_infrastructure>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - checkout
      - check_master
      - install_ansible
      - configure_ansible
      - run: 
          name: Add back-end ip to inventory
          command: |
            # Access secrets which can be rotated by the authorized IAM user
            # without the need of manually deleting and adding new env variables in Circleci
            
            EC2_USER=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleSSH \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."user")

            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.resourceID>>') 

            # Add back-end connection details to ansible inventory
            # default host inventory /etc/ansible/hosts
            # ansible_ssh_private_key_file=./web-key.pem

            if [[ $EC2_HOST != "" && -f ".circleci/ansible/inventory.ini" ]]; then
              echo "web ansible_host=$EC2_HOST ansible_connection=ssh  ansible_user=$EC2_USER" | tee -a  .circleci/ansible/inventory.ini >> /dev/null
            fi          
            chmod 0600 ".circleci/ansible/inventory.ini"

            # cleanup
            unset EC2_USER
            unset EC2_HOST        
      - run:
          name: Configure server
          command: |
            #        
            ansible-playbook  -i .circleci/ansible/inventory.ini .circleci/ansible/configure-server.yml
      - persist_to_workspace:
          root: .
          paths:
            - .circleci/ansible/inventory.ini
      - run:
          name: Cleanup aws config
          command: |
            # cleanup
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE

      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
  run-migrations:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_migrations>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - checkout
      - check_master   
      # - restore_cache:
      #     keys: 
      #       - v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      #       # fallback to using the latest cache 
      #       - v1-backend-deps-
      - run:
          name: Run migrations
          working_directory: ./backend
          command: |
            export TYPEORM_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")

            export TYPEORM_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password")

            export TYPEORM_DATABASE=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."dbname")

            RDS_PORT=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Port")

            RDS_HOST=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Address")
            export TYPEORM_HOST=localhost
            export TYPEORM_PORT=5532
            export NODE_ENV=test
            # check other environment variables
            if [ -z "${TYPEORM_CONNECTION:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_CONNECTION environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_ENTITIES:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_ENTITIES environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_MIGRATIONS environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS_DIR:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_MIGRATIONS_DIR environment variable must be present."
                exit 1
            fi

            #SSH connection configuration
            if [ "${HOME}" = "/" ]
            then
              export HOME=$(getent passwd $(id -un) | cut -d: -f6)
            fi
            printf "%s QbM8rDBaZ9yajNovvO09gv+ks71u1c1y0C4S6Bt39CE" $EC2_HOST >> "$HOME/.ssh/known_hosts"
            chmod 0600 "$HOME/.ssh/known_hosts"
            if [ -f "$HOME/.ssh/id_ed25519" ]; then
              rm -f "$HOME/.ssh/id_ed25519"
            fi

            #access ssh key from aws secrets
            EC2_USER=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleSSH \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."user")

            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.resourceID>>') 

            aws ssm get-parameter \
                --name /aws/reference/secretsmanager/udapeople_ssh_key \
                --with-decryption --output text --query "Parameter.Value" > "$HOME/.ssh/id_ed25519"

            chmod 0700 "$HOME/.ssh/id_ed25519"
            # ssh tunnel
            ssh -4 -i $HOME/.ssh/id_ed25519 -o StrictHostKeyChecking=no -f -N -L $TYPEORM_PORT:$RDS_HOST:$RDS_PORT $EC2_USER@$EC2_HOST 
            npm cache clean --force
            npm i
            npm run migrations >> migrations.txt

            # cleanup
            unset TYPEORM_USERNAME
            unset TYPEORM_PASSWORD
            unset TYPEORM_DATABASE
            unset RDS_HOST
            unset RDS_PORT
            #killall ssh
            rm -f $HOME/.ssh/id_ed25519
      - run: 
          name: Upload migration results
          working_directory: ./backend
          command: |
            aws s3 cp \
              ./migrations.txt s3://$CFN_BUCKET/migrations.txt 
      - run: 
          name: Run migrations tests
          working_directory: ./backend
          command: |
            # Assumptions: All migrations in $TYPEORM_MIGRATIONS_DIR have not yet been executed
            # and therefore are not present in the postgres database. To remove this assumption
            # this command will have to parse the migrations report for any evidence of migrations
            # already done. Or query the database of such migrations before testing.

            # get migrations test details
            ls $TYPEORM_MIGRATIONS_DIR | sed -e s/-/''/ -e s/.ts/''/ -e 's/\([0-9]*\) *\(.*\)/\2\1/' -e 's/^/Migration /g' -e 's/$/ has been executed successfully./g' >> /tmp/MIGRATIONS_TESTS

            # get number of tests to perform
            NUMBER_OF_TESTS=$(grep -c ^ /tmp/MIGRATIONS_TESTS)

            # define test function
            GREEN='\033[0;32m'       
            RED='\033[0;31m'        
            NC='\033[0m' 
            test_migrations () {
            PARSE_RESULT=$(grep -Fx "$1" $2 >/dev/null; echo $?)
            echo $PARSE_RESULT

            case $PARSE_RESULT in
            0)
              printf "TEST: ${GREEN}PASSED${NC}\n"
              ;;
            *)
              printf "TEST: ${RED}FAILED${NC}\n"
              # trigger revert migrations
              #exit 1
              ;;           
            esac
            }
            n=$(seq $NUMBER_OF_TESTS)
            for i in $n
            do
              # define expected expression in migrations report
              # e.g "Migration AddOrders1549375960026 has been executed successfully"
              TEST_PASSED=$(sed -n "$i"p $MIGRATIONS_TESTS)
              echo $TEST_PASSED
              # parse migration report and assert expected expression
              test_migrations "$TEST_PASSED" $MIGRATIONS_REPORT
            done
      # - save_cache:
      #     paths: 
      #       - backend/node_modules
      #     key: v1-backend-deps-{{ checksum "./backend/package-lock.json" }}
      - run:
          name: Cleanup aws config
          command: |
            # cleanup
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
            TUNNEL_TEST=$(nc -z localhost 5532)
            if [ $TUNNEL_TEST != '' ]; then
              killall ssh &>/dev/null
            fi
            rm -f $HOME/.ssh/id_ed25519
      - run:
          name: Revert migration
          working_directory: ./backend
          command: | 
            export TYPEORM_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")

            export TYPEORM_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password")

            export TYPEORM_DATABASE=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."dbname")

            RDS_PORT=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Port")

            RDS_HOST=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Address")
            export TYPEORM_HOST=localhost
            export TYPEORM_PORT=5532
            export NODE_ENV=test
            # check other environment variables
            if [ -z "${TYPEORM_CONNECTION:-}" ]; then
                echo "In order to use the migrations job a TYPEORM_CONNECTION environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_ENTITIES:-}" ]; then
                echo "In order to use the migrations job a TYPEORM_ENTITIES environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS:-}" ]; then
                echo "In order to use the migrations job a TYPEORM_MIGRATIONS environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS_DIR:-}" ]; then
                echo "In order to use the migrations job a TYPEORM_MIGRATIONS_DIR environment variable must be present."
                exit 1
            fi
            # check if tunnel is still live
            TUNNEL_TEST=$(nc -z localhost 5532)
            if [ $TUNNEL_TEST == '' ]; then
              echo "No ssh tunnel. Could not revert migrations."
              exit 1
            fi
            npm cache clean --force
            npm i
            npm run migrations:revert 
            # cleanup
            unset TYPEORM_USERNAME
            unset TYPEORM_PASSWORD
            unset TYPEORM_DATABASE
            unset TYPEORM_HOST
            unset TYPEORM_PORT
            if [ $TUNNEL_TEST != '' ]; then
              killall ssh &>/dev/null
            fi
            rm -f $HOME/.ssh/id_ed25519
          when: on_fail
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
  deploy-frontend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_deploy_frontend>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_USER_ACCESS_KEY_ID
          secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
      - checkout
      - check_master
      - restore_cache:
          keys: 
            - v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}
            # fallback to using the latest cache 
            - v1-frontend-deps-
      - run:
          name: Get backend url and build
          working_directory: ./frontend
          command: |
            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.resourceID>>') 
            export API_URL="http://${EC2_HOST}:3030"
            npm cache clean --force
            npm i
            npm run build
      - run: 
          name: Deploy frontend objects
          command: |+
            aws s3 sync \
              ./frontend/dist s3://udapeoples3-<<pipeline.parameters.resourceID>>/build-${CIRCLE_WORKFLOW_ID:0:7}/--delete \
              --acl public-read \
              --cache-control "max-age=86400"   
      - save_cache:
          paths: 
            - frontend/node_modules
          key: v1-frontend-deps-{{ checksum "./frontend/package-lock.json" }}   
      - run:
          name: Cleanup aws config
          command: |
            # cleanup
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
      - run: 
          name: Rollback frontend 
          command: |
            aws s3 rm s3://udapeoples3-<<pipeline.parameters.resourceID>>/build-${CIRCLE_WORKFLOW_ID:0:7} --recursive 
          when: on_fail
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify
           
  deploy-backend:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_deploy_backend>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_ADMIN_ACCESS_KEY_ID
          secret_access_key: $AWS_ADMIN_SECRET_ACCESS_KEY
      - checkout
      - check_master
      - install_ansible
      - configure_ansible
      - attach_workspace:
          at: .
      - run: 
          name: build backend
          working_directory: ./backend
          command: |
            npm cache clean --force
            npm i
            npm run build
# to avoid slow ansible.builtin.copy, use ansible.builtin.unarchive 
      - run: 
          name: backup files
          command: tar -czvf backend.tar.gz ./backend
      - run:
          name: Deploy backend
          no_output_timeout: 30m
          command: |
            export TYPEORM_USERNAME=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."username")

            export TYPEORM_PASSWORD=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."password")

            export TYPEORM_DATABASE=$(aws ssm get-parameter \
                --name /aws/reference/secretsmanager/UdaPeopleDB \
                --with-decryption --output text --query "Parameter.Value" | jq -r ."dbname")

            export TYPEORM_PORT=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Port")

            export TYPEORM_HOST=$(aws rds describe-db-instances \
                --db-instance-identifier backendDB-<<pipeline.parameters.resourceID>> \
                --output text --query "DBInstances[*].Endpoint.Address")
            export NODE_ENV=test
            # check other environment variables
            if [ -z "${TYPEORM_CONNECTION:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_CONNECTION environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_ENTITIES:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_ENTITIES environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_MIGRATIONS environment variable must be present."
                exit 1
            fi
            if [ -z "${TYPEORM_MIGRATIONS_DIR:-}" ]; then
                echo "In order to use the Deploy backend job a TYPEORM_MIGRATIONS_DIR environment variable must be present."
                exit 1
            fi
            export LATEST_BUILD_IDENTIFIER=${CIRCLE_WORKFLOW_ID:0:7}
            ansible-playbook  -i <<pipeline.parameters.ans_inventory_file>> .circleci/ansible/deploy-backend.yml

            #cleanup
            unset TYPEORM_USERNAME
            unset TYPEORM_PASSWORD
            unset TYPEORM_DATABASE
            unset TYPEORM_HOST
            unset TYPEORM_PORT
      - run:
          name: Cleanup config
          command: |
            # cleanup aws and ansible cfg
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
            #rm -f <<pipeline.parameters.ans_inventory_file>>
      - run:
          name: rollback backend
          command: |
            export LATEST_BUILD_IDENTIFIER=${CIRCLE_WORKFLOW_ID:0:7}

            ansible-playbook  -i <<pipeline.parameters.ans_inventory_file>> .circleci/ansible/rollback-backend.yml
            # cleanup
            rm -f <<pipeline.parameters.ans_inventory_file>>
          when: on_fail
      - slack/notify:
          event: fail
          template: basic_fail_1
      # - jira/notify

  smoke-test:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_smoke_test>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_USER_ACCESS_KEY_ID
          secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
      - run:
          name: Backend smoke test.
          command: |
            EC2_HOST=$(aws --output text --query 'Reservations[*].Instances[*].PublicIpAddress' \
            ec2 describe-instances --filters Name='tag:Name',Values='backend-<<pipeline.parameters.resourceID>>')
            curl http://$EC2_HOST:3030/api/status | grep "ok" > /tmp/BACKEND_SMOKE_TEST
            echo "Backend smoke test"
            GREEN='\033[0;32m'       
            RED='\033[0;31m'        
            NC='\033[0m' 
            TEST_PASSED=$(cat /tmp/BACKEND_SMOKE_TEST)
            if [[ $TEST_PASSED == "ok" ]]; then
              printf "TEST: ${GREEN}PASSED${NC}\n"
            else
              printf "TEST: ${RED}FAILED${NC}\n"
            fi
      - run:
          name: Frontend smoke test.
          command: |
            URL=http://udapeoples3-<<pipeline.parameters.resourceID>>..s3-website-us-east-1.amazonaws.com/build-${CIRCLE_WORKFLOW_ID:0:7}/#/employees
            curl -S $URL | grep "Welcome" > /tmp/FRONTEND_SMOKE_TEST
            echo "Frontend smoke test"
            GREEN='\033[0;32m'       
            RED='\033[0;31m'        
            NC='\033[0m' 
            TEST_PASSED=$(cat /tmp/FRONTEND_SMOKE_TEST)
            if [[ $TEST_PASSED == "Welcome" ]]; then
              printf "TEST: ${GREEN}PASSED${NC}\n"
            else
              printf "TEST: ${RED}FAILED${NC}\n"
            fi
      - run:
          name: Cleanup config
          command: |
            # cleanup aws 
            rm -f $AWS_SHARED_CREDENTIALS_FILE
            rm -f $AWS_CONFIG_FILE
      # Here's where you will add some code to rollback on failure  

  cloudfront-update:
    <<: *defaults
    steps:
      - check_job:
          start_job: <<pipeline.parameters.run_cloudfront_update>>
      - install_aws
      - configure_aws:
          access_key_id: $AWS_USER_ACCESS_KEY_ID
          secret_access_key: $AWS_USER_SECRET_ACCESS_KEY
      - run:
          name: Update cloudfront distribution
          command: |
            S3_LATEST_BUILD_FOLDER="build-${CIRCLE_WORKFLOW_ID:0:7}"
            # get distribution
            aws cloudfront get-distribution --id $PARAM_DISTRIBUTION_ID > /tmp/cloudfront.json

            # get Etag
            ETAG=$(jq -r '.Etag' cloudfront.json)

            # save config

            # change config
            UPDATE_CONFIG=$(jq ".Distribution.DistributionConfig                    |
              (select(.Origins.Items[][\"Id\"] == \"$PARAM_CLOUDFRONT_ORIGIN_ID\")  |
              .Origins.Items[].OriginPath) = \"/$S3_LATEST_BUILD_FOLDER\" " /tmp/cloudfront.json)
            
            # update
            aws cloudfront update-distribution --id $PARAM_DISTRIBUTION_ID --if-match $ETAG \
              --distribution-config $UPDATE_CONFIG
          environment: 
            PARAM_DISTRIBUTION_ID: <<pipeline.parameters.cloudfront_distribution_id>>
            PARAM_CLOUDFRONT_ORIGIN_ID: <<pipeline.parameters.cloudfront_origin_id>>

      # Here's where you will add some code to rollback on failure  

# cleanup:
#     docker:
#       # Docker image here
#     steps:
#       # Checkout code from git
#       - run:
#           name: Get old stack workflow id
#           command: |
#             # your code here
#             export OldWorkflowID="the id here"
#             export STACKS=[] #put the list of stacks here
#       - run:
#           name: Remove old stacks and files
#           command: |
#             if [[ "${STACKS[@]}" =~ "${OldWorkflowID}" ]]
#             then
#               # your code here
#             fi
workflows:
  default:
    jobs:
      - build-frontend:
          pre-steps: # Check commit message if NO_BUILD
            - cancel-workflow      
      - build-backend:
          pre-steps: # Check commit message if NO_BUILD
            - cancel-workflow
      - test-frontend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-frontend
          # filters:
          #   branches:
          #     only: master
      - test-backend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-backend
          # filters:
          #   branches:
          #     only: master
      - scan-backend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-backend
      - scan-frontend:
          post-steps:
            - jira/notify:
                environment_type: testing
                job_type: build
          requires: 
            - build-frontend
      - scan-sonar:
          post-steps:
            - when:
                condition: on_fail
                steps:
                - jira/notify:
                    environment_type: testing
                    job_type: build
          requires: 
            - build-frontend
            - build-backend
      - deploy-infrastructure:
          requires: 
            - test-frontend
            - test-backend
            - scan-frontend
            - scan-backend
          context:
            - org-global
            - aws-context
      - configure-infrastructure:
          requires: 
            - deploy-infrastructure
          context:
            - org-global
            - aws-context
      - run-migrations:
          requires: 
            - configure-infrastructure
          context:
            - org-global
            - aws-context
      - deploy-frontend:
          requires: 
            - run-migrations
          context:
            - org-global
            - aws-context
      - deploy-backend:
          requires: 
            - run-migrations
          context:
            - org-global
            - aws-context
      - smoke-test:
          requires: 
            - deploy-backend
            - deploy-frontend
      - slack/on-hold:
          requires:
            - smoke-test
      - approve-update:
          type: approval
          requires: 
            - smoke-test
      - cloudfront-update:
          requires: 
            - approve-update
      # - cleanup:
      #     requires: [cloudfront-update]
  nightly:
    jobs:
      - scan-frontend
      - scan-backend
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
  # integration_tests:
  #   when: << pipeline.parameters.run_integration_tests >>
  #   jobs:
  #     - build-frontend
  #     - build-backend
  #     - deploy-infrastructure:
  #         requires: 
  #           - build-frontend
  #           - build-backend
  #         context:
  #           - org-global
  #           - aws-context
  #     - configure-infrastructure:
  #         requires: 
  #           - deploy-infrastructure
  #     - run-migrations:
  #         requires: 
  #           - configure-infrastructure
  #     - deploy-frontend:
  #         requires: 
  #           - run-migrations
  #     - deploy-backend:
  #         requires: 
  #           - run-migrations
  #     - cloudfront-update:
  #         requires: 
  #         - run-migrations
  #     - cleanup:
  #         requires: 
  #           - cloudfront-update